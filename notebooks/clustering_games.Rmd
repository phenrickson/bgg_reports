---
title: "Comparing Boardgames via Unsupervised Learning"
output: 
  html_document:
    toc: TRUE #adds a Table of Contents
    number_sections: TRUE #number your headings/sections
    toc_float: TRUE #let your ToC follow you as you scroll
    keep_md: no
    fig.caption: yes
params:
  end_training_year: 2020
  min_ratings: 30
---

```{r global settings, echo=F, warning=F, message=F, results='hide'}

knitr::opts_chunk$set(echo = F,
                      error=F,
                      dev="png",
                      fig.width = 10,
                      fig.height = 6)

options(knitr.duplicate.label = "allow")

options(scipen=999)

# load packages to be used
source(here::here("scripts/load_packages.R"))

# additional libraries
# parallel
library(doParallel)
library(parallelly)

# for stan
library(brms)
library(broom.mixed)

# load custom functions to be used
source(here::here("functions/theme_phil.R"))
source(here::here("functions/tidy_name_func.R"))
source(here::here("functions/pivot_and_dummy_types.R"))
rm(a)

```

```{r flextable settings, echo=F, warning=F, message=F, results='hide'}

#library(webshot2)
library(flextable)
set_flextable_defaults(theme_fun = theme_alafoli,
                       font.color = "grey10",
  padding.bottom = 6, 
  padding.top = 6,
  padding.left = 6,
  padding.right = 6,
  background.color = "white")

```

```{r connect to big query and query tables we need, warning=F, message=F, results='hide'}

library(bigrquery)

# get project credentials
PROJECT_ID <- "gcp-analytics-326219"
BUCKET_NAME <- "test-bucket"

# authorize
bq_auth(email = "phil.henrickson@aebs.com")

# establish connection
bigquerycon<-dbConnect(
        bigrquery::bigquery(),
        project = PROJECT_ID,
        dataset = "bgg"
)

# query table of game info to most recent load
active_games<-DBI::dbGetQuery(bigquerycon, 
                              'SELECT * FROM bgg.api_game_info
                              where timestamp = (SELECT MAX(timestamp) as most_recent FROM bgg.api_game_info)') %>%
        select(-starts_with("rank")) %>%
        mutate(numweights = as.numeric(numweights)) %>%
        mutate_at(c("averageweight",
                    "playingtime",
                    "minplaytime",
                    "maxplaytime",
                    "yearpublished"),
                  ~ case_when(. == 0 ~ NA_real_,
                              TRUE ~ .))

# ugh, made a mistake in the schema...

# create caption for plots
my_caption = list(labs(caption = paste(paste("Data from boardgamegeek.com as of", max(as.Date(active_games$timestamp))),
                        paste("Data and analysis at github.com/phenrickson/bgg"), sep="\n")))


# long table with game type variables
game_types= DBI::dbGetQuery(bigquerycon, 
                              'SELECT * FROM bgg.api_game_categories')

```

# What is this Analysis? {-}

This notebook details my approach to clustering board games and finding nearest neighbors.

For this analysis, we will restrict ourserlves to games published through `r params$end_training_year`. We will validate the performance of our models by evaluating their performance in predicting games published in `r params$end_training_year + 1`.

# The Data

```{r get categorical features}

# load in categorical features
categorical_features_selected = readr::read_rds(here::here("data", "categorical_features_selected.Rdata"))

# select in full game types set
game_types_selected = game_types %>%
        left_join(., categorical_features_selected %>%
                          select(type, id, value, tidied, selected),
                  by = c("type", "id", "value")) %>%
        filter(selected == 'yes')

# pivot and spread these out
game_types_pivoted =game_types_selected %>%
        select(game_id, type, value) %>%
        mutate(type_abbrev = substr(type, 1, 3)) %>%
        mutate(value = tolower(gsub("[[:space:]]", "_", gsub("\\s+", " ", gsub("[[:punct:]]","", value))))) %>%
        mutate(type = paste(type, value, sep="_")) %>%
        mutate(has_type = 1) %>%
        select(-value) %>%
        pivot_wider(names_from = c("type"),
                            values_from = c("has_type"),
                            id_cols = c("game_id"),
                            names_sep = "_",
                            values_fn = min,
                            values_fill = 0)

# now join
games_model = active_games %>%
        left_join(.,
                  game_types_pivoted,
                  by = "game_id") 

```

## Splitting Data

```{r prepare datasets for modeling, echo=T}

# get full dataset
games_full = games_model %>%
        mutate(dataset = case_when(yearpublished <= params$end_training_year ~ 'train',
                                   yearpublished == params$end_training_year+1 | yearpublished == params$end_training_year +2 ~ 'validation',
                                   TRUE ~ 'test')) %>%
        mutate(log_usersrated = log(usersrated))

# filter our training set to only games with at least n ratings
games_train = games_full %>%
        filter(dataset == 'train') %>%
        filter(usersrated >= params$min_ratings)

games_validation = games_full %>%
        filter(dataset == 'validation')

games_test =  games_full %>%
        filter(dataset == 'test')

# count up number of games in each
bind_rows(games_train,
          games_validation,
          games_test) %>%
        group_by(dataset) %>%
        count() %>%
        arrange(desc(n)) %>%
        rename(games = n) %>%
        flextable() %>%
        autofit()

rm(game_types_pivoted,
   game_types_selected,
   game_types)
```


```{r build recipe, echo=T}

# creating recipe with no formula or outcome specified yet
base_recipe = recipe(x = games_train) %>%
        update_role(all_numeric(),
                    new_role = "predictor") %>%
        step_mutate_at(c("averageweight"),
                         fn = ~ na_if(., 0),
                       skip = T) %>% # set to skip as this will be an outcome
        step_mutate_at(c("yearpublished",
                         "playingtime"),
                       fn = ~ na_if(., 0)) %>% # these variables come through as 0 if they are missing
        update_role(one_of("timestamp",
                           "yearpublished",
                      "dataset",
                      "game_id",
                      "name",
                      "numcomments",
                      "numweights",
                      "owned",
                      "trading",
                      "wanting",
                      "wishing",
                      "timestamp",
                      "average",
                      "bayesaverage",
                      "averageweight",
                      "usersrated",
                      "log_usersrated",
                      "usersrated",
                      "stddev"),
                      new_role = "id") %>%
        step_filter(!is.na(yearpublished)) %>%
        step_filter(!is.na(name)) %>%
        step_mutate(missing_minage = case_when(is.na(minage) ~ 1,
                                               TRUE ~ 0)) %>%
        step_mutate(missing_playtingtime = case_when(is.na(playingtime) ~ 1,
                                                     TRUE ~ 0)) %>%
        step_impute_median(playingtime,
                           maxplayers,
                           minage) %>% # medianimpute numeric predictors
        # step_mutate(published_prior_1950 = case_when(yearpublished<1950 ~ 1,
        #                                                TRUE ~ 0)) %>%
        step_mutate(minplayers = case_when(minplayers < 1 ~ 1,
                                                     minplayers > 10 ~ 10, # truncate
                                                     TRUE ~ minplayers),
                    maxplayers = case_when(maxplayers < 1 ~ minplayers,
                                                     maxplayers > 20 ~ 20,
                                                     TRUE ~ maxplayers)) %>%
        step_rm(minplaytime, 
                maxplaytime) %>%
        step_mutate(time_per_player = playingtime/ maxplayers) %>% # make time per player variable
        step_mutate_at(starts_with("category_"),
                           fn = ~ replace_na(., 0)) %>%
        step_mutate_at(starts_with("mechanic_"),
                           fn = ~ replace_na(., 0)) %>%
        step_mutate_at(starts_with("artist_"),
                           fn = ~ replace_na(., 0)) %>%
        step_mutate_at(starts_with("designer_"),
                           fn = ~ replace_na(., 0)) %>%
        step_mutate_at(starts_with("publisher_"),
                           fn = ~ replace_na(., 0)) %>%  
        step_mutate_at(starts_with("family_"),
                           fn = ~ replace_na(., 0)) %>%
        step_mutate(number_mechanics = rowSums(across(starts_with("mechanic_"))),
                  #    number_artists = rowSums(across(starts_with("art_"))),
                      number_categories = rowSums(across(starts_with("category_")))) %>%
        step_zv(all_predictors()) %>%
        step_nzv(all_predictors(),
                   -starts_with("publisher_"),
                   -starts_with("artist_"),
                   -starts_with("designer_"),
                   freq_cut = 100/1) %>% 
        step_corr(all_predictors(),
                  threshold = 0.9) %>%
        step_mutate(published_prior_1900 = case_when(yearpublished < 1900 ~ 1,
                                                     TRUE ~ 0)) %>%
        step_mutate(published_prior_1950 = case_when(yearpublished < 1950 ~ 1,
                                                     TRUE ~ 0)) %>%
        step_mutate(trunc_yearpublished = case_when(yearpublished < 1950 ~ 1950,
                                              TRUE ~ yearpublished)) %>% # truncate
        update_role("yearpublished",
                    new_role = "id") %>%
        # step_mutate(cut_yearpublished= yearpublished) %>%
        # step_cut(cut_yearpublished,
        #                      breaks = seq(1970, 2010, 10),
        #                      include_outside_range = T) %>%
        step_mutate(cut_playingtime= playingtime) %>%
        step_cut(cut_playingtime,
                             breaks = c(15, 45, 90, 180),
                             include_outside_range = T) %>%
        update_role("playingtime",
                    new_role = "id") %>%
        step_dummy(all_nominal_predictors()) %>%
        step_log(time_per_player,
                   offset = 1) %>%
        step_dummy(all_nominal_predictors()) %>%
        step_zv(all_predictors()) %>% # remove features with no variance
        step_nzv(all_predictors(),
                   -starts_with("publisher_"),
                   -starts_with("artist_"),
                   -starts_with("designer_"),
                   freq_cut = 100/1) %>% # apply near zero variance filter
        step_nzv(starts_with("artist_"),
          #       -one_of(c("artist_ian_otoole",
           #                "artist_chris_quilliams")), # allow for some specific artists, well known in recent years
                 freq_cut = 250/1) %>%
        step_corr(all_predictors(),
                  threshold = 0.9) %>% # remove highly, highly correlated features 
        update_role(
                c("averageweight"),
                new_role = "predictor") %>% #
        step_impute_linear(averageweight,
                           impute_with = imp_vars(
                                   all_numeric_predictors(),
                                   -starts_with("publisher_"),
                                   -starts_with("artist_"),
                                   starts_with("designer_"),
                                   )) 

```

# PCA

I'm first interested in exploring using PCA for dimension reduction to learn more about the main points of variation in the data.

I'll then create an additional recipe on top of this that normalizes the data for PCA at the end. I'll define workflows that run PCA with and without specific categories of predictors.

```{r build out recipe with pca, echo=T}

# pca focusing on fundamentals + mechanics
pca_mechanics_recipe = base_recipe %>%
        step_rm(starts_with("trunc_yearpublished"),
                starts_with("category"),
                starts_with("artist_"),
                starts_with("designer_"),
                starts_with("publisher_"),
                starts_with("published_"),
                starts_with("family")) %>%
        step_normalize(all_numeric_predictors()) %>%
        step_pca(all_numeric_predictors(), 
                 id = "pca",
                 num_comp = 500)

# pca using categories and family variables as well
pca_categories_recipe = base_recipe %>%
        step_rm(starts_with("trunc_yearpublished"),
                starts_with("artist_"),
                starts_with("designer_"),
                starts_with("publisher_"),
                starts_with("published_")) %>%
        step_normalize(all_numeric_predictors()) %>%
        step_pca(all_numeric_predictors(), 
                 id = "pca",
                 num_comp = 500)
                        
```

We can then bake these recipes on the training set.

```{r bake pca, echo=T}

# prep recipe for categories
pca_categories_trained = pca_categories_recipe %>%
        prep(games_train)

# prep recipes for mechanics
pca_mechanics_trained = pca_mechanics_recipe %>%
        prep(games_train)

# extract componenbts from each of these
pca_categories_components = pca_categories_trained %>%
        tidy(id = "pca") %>%
        mutate(component = gsub("PC0", "PC", gsub("PC00", "PC", component)))

pca_mechanics_components = pca_mechanics_trained %>%
        tidy(id = "pca") %>%
        mutate(component = gsub("PC0", "PC", gsub("PC00", "PC", component)))

```

We've now run PCA on two separate data sets, one which uses BGG categories as features, and the other which focuses on mechanics.

## PCA Loadings

We can look at the loadings for the first two principal components from each PCA.

### Categories

For the PCA that looks at everything, including BGG categories + families, we can see how games are placed on the first two principal components.

```{r  plot the first two main principal components for categories, warning=F}

# define arrow style
arrow_style <- arrow(length = unit(.05, "inches"),
                     type = "closed")

# pca_loadings
pca_wider = pca_categories_components %>%
          tidyr::pivot_wider(names_from = component, id_cols = terms)

# make plot
pca_categories_components %>%
        filter(id == 'pca') %>%
        mutate(abs_value = abs(value)) %>%
        tidyr::pivot_wider(names_from = component, id_cols = terms) %>%
        arrange(desc(abs(PC1))) %>%
        mutate(terms = tidy_name_func(terms)) %>%
        head(100) %>%
        ggplot(., aes(x=PC1, 
                y=PC2))+
        geom_segment(alpha=0.5,
                     aes(xend = PC1, yend = PC2), 
                     x = 0, y = 0, 
                     color = 'grey60',
               arrow = arrow_style)+
          geom_text(aes(x = PC1, y = PC2, label = terms), 
                    check_overlap=T,
            hjust = 0, 
            vjust = 1,
            size = 3)+
        my_caption+
        xlab("First Principal Component")+
        ylab("Second Principal Component")+
        geom_vline(xintercept = 0,
                   linetype = 'dashed')+
        geom_hline(yintercept = 0,
                   linetype = 'dashed')+
        theme_phil()
 
```

The first principal component looks to mostly map to complexity, while the second component tends to map to something close to storytelling/theme/interaction. This means the bottom right quadrant is mostly heavy, highly thematic games like Gloomhaven, Mansions of Madness, the top right quadrant is mostly GMT wargames, the bottom left is mostly party/social deduction games, and the top left is a mix of simpler dexterity/abstract games.

Or, more simply, we can place every game and then label these quadrants.

```{r plot component loadings for categories, fig.height=8, fig.width=10}

pca_categories_rotation = pca_categories_trained %>%
        prep(games_train) %>%
        bake(new_data = NULL) %>%
        set_names(., gsub("PC0", "PC", gsub("PC00", "PC", names(.))))

pca_categories_rotation %>%
        ggplot(., aes(x=PC1, 
                label = name,
                y=PC2))+
        geom_point(alpha=0.15)+
        geom_text(check_overlap = T,
             size = 2,
             vjust = -1)+
        #   position=position_jitter(width=0.2,height=0.2))+
        theme_phil()+
        my_caption+
        xlab("First Principal Component")+
        ylab("Second Principal Component")+
        geom_vline(xintercept = 0,
                   linetype = 'dashed')+
        geom_hline(yintercept =0,
                   linetype = 'dashed')+
        annotate("label",
                 label = "Complex, Thematic",
                 color = 'blue',
                 x = 9,
                 y = -8)+
        annotate("label",
                 label = "Abstract",
                 color = 'blue',
                 x = -4,
                 y = 5)+
        annotate("label",
                 label = "Party, Social Deduction",
                 color = 'blue',
                 x = -5,
                 y = -5)+
        annotate("label",
                 label = "Wargames/Simulation",
                 color = 'blue',
                 x = 10,
                 y = 7)+
        coord_cartesian(xlim = c(-7, 15),
                        ylim = c(-14, 8))

```
How well does each dimension summarize the variation in the data? We can use a scree plot to examine the percentage of variation explained by each component as well as the cumulative variation as we up the number of compnents.

```{r show scree plot for categories}

pca_categories_trained %>%
        tidy(id = "pca",
             type = "variance") %>%
        mutate(outcome = "fundamentals, mechanics, categories") %>%
        filter(terms %in% c("percent variance",
                            "cumulative percent variance")) %>%
        ggplot(., aes(x=component,
                      y=value))+
        geom_col()+
        facet_grid(terms ~ outcome,
                #   ncol =1,
                   scales = "free")+
        theme_phil()
```

Generally speaking, the first few components explain the most of the variation, and it takes about 10-15 components to explain 25% of the overall variation in the dataset.

What contributes to each of those components? We can plot the top 20 variables that contribute the most to each component for the first 10 principal components.

```{r pca loadings for top ten components, fig.height=10, fig.width=10}

number_pcs = paste("PC", seq(1, 10, 1), sep="")

pca_categories_components %>%
        filter(component %in% number_pcs) %>%
        mutate(component = factor(component,
                                  levels = number_pcs)) %>%
        group_by(component) %>%
        slice_max(.,
                  order_by = abs(value),
                  n = 20,
                  with_ties = F) %>%
        mutate(terms = tidy_name_func(terms)) %>%
        ggplot(., aes(x=value,
                      fill = value,
                      y = reorder_within(terms, value, component)))+
        geom_col()+
        facet_wrap(component~.,
                   scales = "free_y",
                   ncol = 2)+
        theme_bw(8)+
        scale_y_reordered()+
        scale_fill_viridis()+
        guides(fill = "none")+
        ylab("")

```

This requires quite a bit of subject matter expertise to digest and interpret what each of these are picking up. I shall give it a go:

The first looks to be, as noted earlier, complexity.
The second looks to about the difference between storytelling games vs simulation heavy wargames.
The third is the difference between tight economic games and games with more luck and storytelling. 
The fourth is straight up about the number of players really needed for the game (solo vs party).
...

I'm struggling with some of these latter ones. I'm going to plot every game on each one of these and pick out particular games, that might help.

```{r plot distribution of games by each component, fig.height=14, fig.width=10}

library(ggforce)

set.seed(1999)
pos <- position_jitternormal(sd_x = 0.1, sd_y = 0.075)
pos2 <- position_jitternormal(sd_x = 0.025, sd_y = 0.025)

# plot with jitter
pca_categories_rotation %>%
        select(game_id, name,
               all_of(number_pcs)) %>%
        gather("component", "value",
               -game_id, -name) %>%
        mutate(component = factor(component,
                                 levels = rev(number_pcs))) %>%
        filter(component %in% paste("PC", seq(1,10), sep="")) %>%
        ggplot(., aes(x=component,
                      label = name,
                      y=value))+
        geom_point(alpha=0.15,
                   position = pos)+
        geom_text(position = pos,
                  check_overlap=T,
                  size =2)+
        coord_flip()+
        theme_phil()+
        geom_hline(yintercept = 0,
                   linetype = 'dashed')

```

Where does a game like Gloomhaven fall in each of these distributions?

```{r pick out individual games in these components}

# create function to place games in distribution
place_game_func = function(rotation,
                           input_pcs,
                           input_game_id)
{
        # create vector
        number_pcs = paste("PC", seq(1, input_pcs), sep="")

        # make initial plot
        a = rotation %>%
        select(game_id, name,
               all_of(number_pcs)) %>%
        gather("component", "value",
               -game_id, -name) %>%
        mutate(component = factor(component,
                                  levels = rev(number_pcs))) %>%
        ggplot(., aes(y=component,
                      x=value))+
        geom_density_ridges(scale = 0.9,
                            color = NA,
                            alpha = 0.75)+
        theme_phil()+
                geom_vline(xintercept =0,
                           linetype = 'dotted')
        
        # now overlay lines
        game_dat = rotation %>%
                           select(game_id, name,
               all_of(number_pcs)) %>%
                       gather("component", "value",
               -game_id, -name) %>%
                       mutate(component = factor(component,
                                  levels = rev(number_pcs))) %>%
                       filter(game_id %in% input_game_id)
        
        # plot vertical lines
        b = a + geom_segment(data = game_dat,
               aes(x=value,
                   xend = value,
                   y = as.numeric(component),
                   color = name,
                   yend=as.numeric(component)+0.5),
               size = 1.1)+
                scale_color_viridis_d(option = "A")
        
        # plot a smooth line through
        c = b + geom_path(data = game_dat,
               aes(x=value,
                   y = as.numeric(component)+0.5,
                   color = name),
               alpha = 0.5,
            #   stat = "smooth",
               lwd = 0.8)
   #     ggtitle(paste(game_dat$name))

        suppressMessages({
                print(c)
        })
        
}

place_game_func(pca_categories_rotation,
                input_pcs = 20,
                input_game_id = active_games %>%
                        filter(name == 'Gloomhaven') %>%
                        pull(game_id))

```

What about a game like, Tigris and Euphrates? This is interesting, Gloomhaven is very far out in the tails on PC1 and PC2, but on opposite sides. PC3 is where the two are very different, which makes if this component is about storytelling vs tight economy.

```{r place another sample game}

place_game_func(pca_categories_rotation,
                20,
                input_game_id = active_games %>% 
                        filter(name == 'Tigris & Euphrates' | name == 'Gloomhaven') %>%
                        pull(game_id))

```

One more, what about a party game like Spyfall?

```{r place another sample game 2}

place_game_func(pca_categories_rotation,
                20,
                input_game_id = active_games %>%
                        filter(name == 'Tigris & Euphrates' |
                                       name == 'Gloomhaven' | 
                                       name == 'Spyfall') %>%
                        pull(game_id))

```

Okay, just *one* more. What about a dudes on a map game like Kemet and a co op like Spirit Island?

```{r place another sample game 2}

place_game_func(pca_categories_rotation,
                20,
                input_game_id = active_games %>%
                        filter(name == 'Tigris & Euphrates' |
                                       name == 'Gloomhaven' | 
                                       name == 'Inis' |
                                       name == 'Spyfall' |
                                       name == 'Spirit Island') %>%
                        pull(game_id))

```
Hmm. I'm going to keep this in mind for later on, this is rather interesting.

### Mechanics

Let's now take a look at the PCA that didn't include BGG categories. 

```{r  plot the first two main principal components for components, warning=F}

# define arrow style
arrow_style <- arrow(length = unit(.05, "inches"),
                     type = "closed")

# pca_loadings
pca_wider = pca_mechanics_components %>%
          tidyr::pivot_wider(names_from = component, id_cols = terms)

# make plot
pca_mechanics_components %>%
        filter(id == 'pca') %>%
        mutate(abs_value = abs(value)) %>%
        tidyr::pivot_wider(names_from = component, id_cols = terms) %>%
        arrange(desc(abs(PC1))) %>%
        mutate(terms = tidy_name_func(terms)) %>%
        head(100) %>%
        ggplot(., aes(x=PC1, 
                y=PC2))+
        geom_segment(alpha=0.5,
                     aes(xend = PC1, yend = PC2), 
                     x = 0, y = 0, 
                     color = 'grey60',
               arrow = arrow_style)+
          geom_text(aes(x = PC1, y = PC2, label = terms), 
                    check_overlap=T,
            hjust = 0, 
            vjust = 1,
            size = 3)+
        my_caption+
        xlab("First Principal Component")+
        ylab("Second Principal Component")+
        geom_vline(xintercept = 0,
                   linetype = 'dashed')+
        geom_hline(yintercept = 0,
                   linetype = 'dashed')+
        theme_phil()
 
```

Based on this, we still get pretty similar groupings, with wargames now occupying the right/upper right, then more thematic games down the bottom middle, party games in the bottom left, and abstracts in upper left.

```{r plot component loadings for mechanics, fig.height=8, fig.width=10}

pca_mechanics_rotation = pca_mechanics_trained %>%
        prep(games_train) %>%
        bake(new_data = NULL) %>%
        set_names(., gsub("PC0", "PC", gsub("PC00", "PC", names(.))))

pca_mechanics_rotation %>%
        ggplot(., aes(x=PC1, 
                label = name,
                y=PC2))+
        geom_point(alpha=0.15)+
        geom_text(check_overlap = T,
             size = 2,
             vjust = -1)+
        #   position=position_jitter(width=0.2,height=0.2))+
        theme_phil()+
        my_caption+
        xlab("First Principal Component")+
        ylab("Second Principal Component")+
        geom_vline(xintercept = 0,
                   linetype = 'dashed')+
        geom_hline(yintercept =0,
                   linetype = 'dashed')
        # annotate("label",
        #          label = "Complex, Thematic",
        #          color = 'blue',
        #          x = 9,
        #          y = -8)+
        # annotate("label",
        #          label = "Abstract",
        #          color = 'blue',
        #          x = -4,
        #          y = 5)+
        # annotate("label",
        #          label = "Party, Social Deduction",
        #          color = 'blue',
        #          x = -5,
        #          y = -5)+
        # annotate("label",
        #          label = "Wargames/Simulation",
        #          color = 'blue',
        #          x = 10,
        #          y = 7)+
        # coord_cartesian(xlim = c(-7, 15),
        #                 ylim = c(-14, 8))

```

How well does each dimension summarize the variation in the data? We can use a scree plot to examine the percentage of variation explained by each component as well as the cumulative variation as we up the number of compnents.

```{r show scree plot for mechanics}

pca_mechanics_trained %>%
        tidy(id = "pca",
             type = "variance") %>%
        mutate(outcome = "fundamentals, mechanics") %>%
        filter(terms %in% c("percent variance",
                            "cumulative percent variance")) %>%
        ggplot(., aes(x=component,
                      y=value))+
        geom_col()+
        facet_grid(terms ~ outcome,
                #   ncol =1,
                   scales = "free")+
        theme_phil()
```

Here, the first few components explain more of the variation, and it takes about 7 components to explain 25% of the overall variation in the dataset.

What contributes to each of those components? We can plot the top 20 variables that contribute the most to each component for the first 10 principal components.

```{r pca loadings for top ten components, fig.height=10, fig.width=10}

number_pcs = paste("PC", seq(1, 10, 1), sep="")

pca_mechanics_components %>%
        filter(component %in% number_pcs) %>%
        mutate(component = factor(component,
                                  levels = number_pcs)) %>%
        group_by(component) %>%
        slice_max(.,
                  order_by = abs(value),
                  n = 20,
                  with_ties = F) %>%
        mutate(terms = tidy_name_func(terms)) %>%
        ggplot(., aes(x=value,
                      fill = value,
                      y = reorder_within(terms, value, component)))+
        geom_col()+
        facet_wrap(component~.,
                   scales = "free_y",
                   ncol = 2)+
        theme_bw(8)+
        scale_y_reordered()+
        scale_fill_viridis()+
        guides(fill = "none")+
        ylab("")

```

We look to be getting some similar results.

```{r place sample games for mechanics}

place_game_func(pca_mechanics_rotation,
                10,
                input_game_id = active_games %>% 
                        filter(name == 'Tigris & Euphrates' | 
                                       name == 'Nemesis' |
                                       name == 'Spirit Island' |
                                       name == 'Gloomhaven') %>%
                        pull(game_id))

```

```{r remove unnecessary objects from memory, warning=F, message=F}

rm(games_model,
   categorical_features_selected,
   pca_wider,
   base_recipe)

gc()

```


# Nearest Neighbors

Once we've computed the principal components using the different sets of features, we can get the distance between all observations in order to find each game's nearest neighbors.

